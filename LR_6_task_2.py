# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18hXgY2pXBVdmm6LRDUDbZDL9MMFwI1QW
"""

# Імпортуємо необхідні бібліотеки
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
from sklearn.preprocessing import MinMaxScaler

# Створення сигналу
i1 = np.sin(np.arange(0, 20))
i2 = np.sin(np.arange(0, 20)) * 2

t1 = np.ones([1, 20])
t2 = np.ones([1, 20]) * 2

# Підготовка вхідних та цільових даних
input_signal = np.array([i1, i2, i1, i2]).reshape(80, 1)
target_signal = np.array([t1, t2, t1, t2]).reshape(80, 1)

# Нормалізація даних
scaler_in = MinMaxScaler(feature_range=(-1, 1))
scaler_out = MinMaxScaler(feature_range=(-1, 1))
input_signal_scaled = scaler_in.fit_transform(input_signal)
target_signal_scaled = scaler_out.fit_transform(target_signal)

# Перетворення до 3D формату для RNN: [samples, time steps, features]
X = input_signal_scaled.reshape((80, 1, 1))
y = target_signal_scaled

# Побудова рекурентної моделі
model = Sequential()
model.add(SimpleRNN(10, activation='tanh', input_shape=(1, 1)))  # прихований шар
model.add(Dense(1))  # вихідний шар

model.compile(optimizer='adam', loss='mse')

# Навчання мережі
history = model.fit(X, y, epochs=500, verbose=0)

# Прогнозування
output = model.predict(X)

# Зворотне перетворення до початкового масштабу
output_inverse = scaler_out.inverse_transform(output)

# Побудова графіків
plt.figure(figsize=(10, 6))

plt.subplot(2, 1, 1)
plt.plot(history.history['loss'])
plt.xlabel('Номер епохи')
plt.ylabel('Помилка навчання (MSE)')
plt.title('Графік помилки навчання')

plt.subplot(2, 1, 2)
plt.plot(target_signal, label='Очікувані значення')
plt.plot(output_inverse, label='Прогнозовані значення')
plt.legend()
plt.title('Апроксимація сигналу RNN')
plt.tight_layout()
plt.show()